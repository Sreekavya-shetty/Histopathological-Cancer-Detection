# -*- coding: utf-8 -*-
"""MajorFinal

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4sPCwFBYCqVBFCYMuMxJs1p024yZa-L
"""

# Commented out IPython magic to ensure Python compatibility.
N from numpy.random import seed
seed(101)

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam

import os
import cv2

from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import itertools
import shutil
import matplotlib.pyplot as plt
# %matplotlib inline


img_size = 96
img_channels = 3

no_of_samples = 80000 # the number of images from each class

from google.colab import drive
drive.mount('/content/drive')

!mkdir proj

!unzip "/content/drive/My Drive/histopathologic-cancer-detection.zip" -d "/content/proj"

print(len(os.listdir('/content/proj/train')))
print(len(os.listdir('/content/proj/test')))

df_data = pd.read_csv('/content/proj/train_labels.csv')



print(df_data.shape)

df_data['label'].value_counts()

Path= '/content/proj/train/'

def draw_category_images(col_name,figure_cols, df, Path):
    
    

    categories = (df.groupby([col_name])[col_name].nunique()).index
    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, 
                         figsize=(4*figure_cols,4*len(categories)))
    for i, cat in enumerate(categories):
        sample = df[df[col_name]==cat].sample(figure_cols) 
        for j in range(0,figure_cols):
            file=Path + sample.iloc[j]['id'] + '.tif'
            im=cv2.imread(file)
            ax[i, j].imshow(im, resample=True, cmap='gray')
            ax[i, j].set_title(cat, fontsize=16)  
    plt.tight_layout()
    plt.show()

draw_category_images('label',4, df_data, Path)

df_data.head()

df_0 = df_data[df_data['label'] == 0].sample(no_of_samples, random_state = 101)

df_1 = df_data[df_data['label'] == 1].sample(no_of_samples, random_state = 101)


df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)

df_data = shuffle(df_data)

df_data['label'].value_counts()

y = df_data['label']

df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)

print(df_train.shape)
print(df_val.shape)

df_train['label'].value_counts()

df_val['label'].value_counts()

main_dir = 'main_dir'
os.mkdir(main_dir)

train_dir = os.path.join(main_dir, 'train_dir')
os.mkdir(train_dir)
val_dir = os.path.join(main_dir, 'val_dir')
os.mkdir(val_dir)

normal = os.path.join(train_dir, 'no_tumor')
os.mkdir(normal)
tumor = os.path.join(train_dir, 'has_tumor')
os.mkdir(tumor)

normal = os.path.join(val_dir, 'no_tumor')
os.mkdir(normal)
tumor= os.path.join(val_dir, 'has_tumor')
os.mkdir(tumor)

os.listdir('main_dir/train_dir')

df_data.set_index('id', inplace=True)

train_imgs = list(df_train['id'])
val_imgs = list(df_val['id'])

for image in train_imgs:
    
    
    fname = image + '.tif'
    target = df_data.loc[image,'label']
    
    
    if target == 0:
        label = 'no_tumor'
    if target == 1:
        label = 'has_tumor'
    
    src = os.path.join('/content/proj/train', fname)
    
    dst = os.path.join(train_dir, label, fname)
    
    shutil.copyfile(src, dst)

for image in val_imgs:
    
    
    fname = image + '.tif'
    
    target = df_data.loc[image,'label']
    
  
    if target == 0:
        label = 'no_tumor'
    if target == 1:
        label = 'has_tumor'
    

    
    src = os.path.join('/content/proj/train', fname)
    
    dst = os.path.join(val_dir, label, fname)
    
    shutil.copyfile(src, dst)

train_path = 'main_dir/train_dir'
valid_path = 'main_dir/val_dir'
test_path = '/content/proj/test'

no_train_samples = len(df_train)
no_val_samples = len(df_val)
train_batch_size = 32
val_batch_size = 32


train_steps = np.ceil(no_train_samples / train_batch_size)
val_steps = np.ceil(no_val_samples / val_batch_size)

datagen = ImageDataGenerator(rescale=1.0/255)

train_gen = datagen.flow_from_directory(train_path,
                                        target_size=(img_size,img_size),
                                        batch_size=train_batch_size,
                                        class_mode='categorical')

val_gen = datagen.flow_from_directory(valid_path,
                                        target_size=(img_size,img_size),
                                        batch_size=val_batch_size,
                                        class_mode='categorical')


test_gen = datagen.flow_from_directory(valid_path,
                                        target_size=(img_size,img_size),
                                        batch_size=1,
                                        class_mode='categorical',
                                        shuffle=False)

kernel_size = (3,3)
pool_size= (2,2)
first_filters = 32
second_filters = 64
third_filters = 128

dropout_conv = 0.3
dropout_dense = 0.3


model = Sequential()
model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))
model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))
model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))
model.add(MaxPooling2D(pool_size = pool_size)) 
model.add(Dropout(dropout_conv))

model.add(Conv2D(second_filters, kernel_size, activation ='relu'))
model.add(Conv2D(second_filters, kernel_size, activation ='relu'))
model.add(Conv2D(second_filters, kernel_size, activation ='relu'))
model.add(MaxPooling2D(pool_size = pool_size))
model.add(Dropout(dropout_conv))

model.add(Conv2D(third_filters, kernel_size, activation ='relu'))
model.add(Conv2D(third_filters, kernel_size, activation ='relu'))
model.add(Conv2D(third_filters, kernel_size, activation ='relu'))
model.add(MaxPooling2D(pool_size = pool_size))
model.add(Dropout(dropout_conv))

model.add(Flatten())
model.add(Dense(256, activation = "relu"))
model.add(Dropout(dropout_dense))
model.add(Dense(2, activation = "softmax"))

model.summary()

model.compile(Adam(lr=0.0001), loss='binary_crossentropy', 
              metrics=['accuracy'])

print(val_gen.class_indices)

filepath = "model.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, 
                             save_best_only=True, mode='max')

reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, 
                                   verbose=1, mode='max', min_lr=0.00001)
                              
                              
callbacks_list = [checkpoint, reduce_lr]

history = model.fit_generator(train_gen, steps_per_epoch=train_steps, 
                    validation_data=val_gen,
                    validation_steps=val_steps,
                    epochs=20, verbose=1,
                   callbacks=callbacks_list)

model.metrics_names

val_loss, val_acc = \
model.evaluate_generator(test_gen, 
                        steps=len(df_val))

print('val_loss:', val_loss)
print('val_acc:', val_acc)

predictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)

predictions.shape

df_preds = pd.DataFrame(predictions, columns=['normal', 'tumor'])

df_preds.head()

y_true = test_gen.classes


y_pred = df_preds['tumor']

from sklearn.metrics import roc_auc_score

roc_auc_score(y_true, y_pred)

test_labels = test_gen.classes

cm = confusion_matrix(test_labels, predictions.argmax(axis=1))

cm_plot_labels = ['normal', 'tumor']

print(cm)

from sklearn.metrics import classification_report

y_pred_binary = predictions.argmax(axis=1)

report = classification_report(y_true, y_pred_binary, target_names=cm_plot_labels)

print(report)